{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:39:38.707340Z",
     "start_time": "2025-11-29T09:39:38.703383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_path = '/home/manh/code/nlp/src/data/VSMEC/train_VSMEC.csv'\n",
    "test_path = '/home/manh/code/nlp/src/data/VSMEC/test_VSMEC.csv'\n",
    "val_path = '/home/manh/code/nlp/src/data/VSMEC/val_VSMEC.csv'"
   ],
   "id": "7137e871787b6fc8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:39:45.494069Z",
     "start_time": "2025-11-29T09:39:38.752804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"mo_hinh_toan\").getOrCreate()"
   ],
   "id": "afe71d556708758e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/29 16:39:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-29T09:39:45.500115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = spark.read.csv(\n",
    "    train_path,\n",
    "    header=True,\n",
    "    inferSchema=True,       # Tự đoán kiểu cột\n",
    "    multiLine=True,         # Nếu có nhiều dòng trong 1 ô\n",
    "    escape='\"',             # Xử lý dấu ngoặc kép\n",
    "    quote='\"'               # Xử lý dấu ngoặc kép\n",
    ")\n",
    "\n",
    "test_df = spark.read.csv(test_path, header=True)\n",
    "val_df = spark.read.csv(val_path, header=True)"
   ],
   "id": "e83d46b8a960b77d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_df.groupby(\"Emotion\").count().show()",
   "id": "3af9a50c80c98d63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "emotions = ['Anger', 'Surprise', 'Enjoyment', 'Other', 'Fear', 'Sadness', 'Disgust']\n",
    "counts = [391, 242, 1558, 1021, 318, 947, 1071]\n",
    "plt.figure(figsize=(8,5), facecolor='white')  # nền trắng\n",
    "plt.bar(emotions, counts, color=['#FF6B6B','#FFA500','#FFD93D','#C0C0C0','#A569BD','#3498DB','#2ECC71'])\n",
    "plt.title('Phân phối cảm xúc trong VSMEC', fontsize=14)\n",
    "plt.ylabel('Số lượng', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # thêm lưới giúp đọc số liệu\n",
    "plt.show()\n"
   ],
   "id": "b949e8fca0df9562",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.head(5)",
   "id": "b4c8afba1c05eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:36:18.111719Z",
     "start_time": "2025-11-29T09:36:15.227750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text , format = \"text\").split()\n",
    "\n",
    "tokenize_udf = udf(tokenize, ArrayType(StringType()))\n",
    "train_df = train_df.withColumn(\"tokens\" , tokenize_udf(train_df.Sentence))\n",
    "test_df =test_df.withColumn(\"tokens\" , tokenize_udf(test_df.Sentence))\n",
    "val_df = val_df.withColumn(\"tokens\" , tokenize_udf(val_df.Sentence))"
   ],
   "id": "45f99fdb8a23c4a5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.head(5)",
   "id": "83f41550dea90ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:36:19.646450Z",
     "start_time": "2025-11-29T09:36:18.126298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml import pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Emotion\", outputCol=\"label\")\n",
    "indexer = indexer.fit(train_df)\n"
   ],
   "id": "5520ed36763dbfe9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:36:19.773339Z",
     "start_time": "2025-11-29T09:36:19.664253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = indexer.transform(train_df)\n",
    "test_df = indexer.transform(test_df)\n",
    "val_df = indexer.transform(val_df)"
   ],
   "id": "78942de3d401aeeb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TF-IDF + naive bayes",
   "id": "97f680be10112721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# remove stop word for tf-idf",
   "id": "9be799108bae8524",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import  CountVectorizer , IDF\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"spare_vec\" , vocabSize=5000, minDF=5 )\n",
    "idf = IDF(inputCol=\"spare_vec\" , outputCol=\"idf\" , minDocFreq=5)"
   ],
   "id": "c66c0d173b1a28b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages= [cv, idf])\n",
    "pipelineModel = pipeline.fit(train_df)"
   ],
   "id": "a7b4de374c363a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_idf = pipelineModel.transform(train_df)\n",
    "test_idf = pipelineModel.transform(test_df)\n",
    "val_idf = pipelineModel.transform(val_df)"
   ],
   "id": "188c0b2410052343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train naive bayes",
   "id": "65960a7f788fcdbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "nb = NaiveBayes(featuresCol=\"idf\" , labelCol=\"label\" , modelType=\"multinomial\")\n",
    "nb_model = nb.fit(train_idf)"
   ],
   "id": "e91c6c65c0ea6c9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "evaluate model",
   "id": "db1495abaf87f9c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "val dataset evaluation",
   "id": "87b0184837cc974d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "predictions = nb_model.transform(val_idf)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# f1 macro\n",
    "pdf = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "macro_f1 = f1_score(pdf['prediction'], pdf['label'], average=\"macro\")\n",
    "print(\"macro_f1: \"  ,   macro_f1 )"
   ],
   "id": "1342b1624a73bb8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test evaluation",
   "id": "5f51081281b3b67d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "tes_pred = nb_model.transform(test_idf)\n",
    "y_pred = np.array(tes_pred.select('prediction').rdd.flatMap(lambda x: x).collect()).astype(int)\n",
    "y_true = np.array(test_idf.select('label').rdd.flatMap(lambda x: x).collect()).astype(int)\n"
   ],
   "id": "752d588a4af29f08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tính Accuracy\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Tính F1-Score (Dùng 'macro' để đánh giá công bằng các lớp)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy cuối cùng: {accuracy:.4f}\")\n",
    "print(f\"F1-Macro Score: {f1_macro:.4f}\")\n"
   ],
   "id": "43b18bc48349849b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [\n",
    "    'Anger',     #\n",
    "    'Surprise',  #\n",
    "    'Enjoyment', #\n",
    "    'Other',     #\n",
    "    'Fear',      #\n",
    "    'Sadness',   #\n",
    "    'Disgust'    #\n",
    "]\n",
    "\n",
    "# 1. Tính toán Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 2. Thiết lập kích thước đồ thị\n",
    "plt.figure(figsize=(10, 8)) # Tăng kích thước để dễ đọc tên lớp\n",
    "\n",
    "# 3. Vẽ biểu đồ nhiệt (Heatmap)\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names, # Sử dụng tên lớp mới\n",
    "    yticklabels=class_names  # Sử dụng tên lớp mới\n",
    ")\n",
    "\n",
    "# 4. Đặt tiêu đề và nhãn\n",
    "plt.title('Confusion Matrix: Phân loại Cảm xúc (Emotion Classification)')\n",
    "plt.ylabel('Nhãn Thực tế (True Emotion)')\n",
    "plt.xlabel('Nhãn Dự đoán (Predicted Emotion)')\n",
    "plt.show()"
   ],
   "id": "d52eb756370a6869",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# word2vec + dense",
   "id": "7e5638d708efa360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.show(2)",
   "id": "ec3ac1a54ea30330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train model embedding word2vec",
   "id": "8ae26598bd8c7d9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences= train_df.select(\"tokens\").rdd.map(lambda row : row.tokens).collect()\n",
    "vocab = [[str(i) for i in sent ] for sent in sentences]\n",
    "w2v = Word2Vec(\n",
    "    sentences = sentences,\n",
    "    vector_size= 100 ,\n",
    "    window=5,\n",
    "    min_count=1\n",
    ")"
   ],
   "id": "b045270cf63e209e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "calculate average vector of sentences",
   "id": "353032592446e7d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "def embedding_ave(sentence):\n",
    "    vecs = []\n",
    "    for word in sentence:\n",
    "        if word in w2v.wv.key_to_index:\n",
    "            vecs.append(w2v.wv[word])\n",
    "        else :\n",
    "            vecs.append(np.zeros(w2v.vector_size))\n",
    "    avg =  np.mean(vecs , axis=0)\n",
    "    return [float(i) for i in avg]\n",
    "udf_emb = udf(embedding_ave , ArrayType(FloatType()) )\n",
    "train_df = train_df.withColumn(\"vector_avg\" , udf_emb(train_df.tokens ))\n",
    "test_df = test_df.withColumn(\"vector_avg\" , udf_emb(test_df.tokens ))\n",
    "val_df = val_df.withColumn('vector_avg' , udf_emb(val_df.tokens ))"
   ],
   "id": "9d849eef329d7062",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.show(2)",
   "id": "8cbda4c437c53769",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset , DataLoader\n",
    "class DenseVectorDataset(Dataset):\n",
    "    def __init__(self , label , vector) :\n",
    "        self.labels = torch.from_numpy(np.array(label)).long()\n",
    "        self.vectors = torch.from_numpy(np.array(vector)).float()\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self , idx):\n",
    "        return  self.vectors[idx] , self.labels[idx]"
   ],
   "id": "6fb1f660fd0f5e10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train loader",
   "id": "eb858d31eea2464"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get labels and vector from train spark dataframe\n",
    "labels_train = train_df.select(\"label\").rdd.map(lambda row : row.label).collect()\n",
    "vectors_train = train_df.select(\"vector_avg\").rdd.map(lambda row : row.vector_avg).collect()\n",
    "train_loader = DataLoader(DenseVectorDataset(labels_train , vectors_train) , batch_size=128 , shuffle=True , num_workers = 4 )"
   ],
   "id": "e48ef8eec8fb5be1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "val loader",
   "id": "8fcc359f852f3167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = val_df.select(\"label\").rdd.map(lambda row : row.label).collect()\n",
    "vectors = val_df.select(\"vector_avg\").rdd.map(lambda row : row.vector_avg).collect()\n",
    "val_loader = DataLoader(DenseVectorDataset(labels , vectors) , batch_size=128 , shuffle=True , num_workers= 4)"
   ],
   "id": "165d92579cdcf60c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test loader",
   "id": "2b5d9b68735a4bbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = test_df.select(\"label\").rdd.map(lambda row : row.label).collect()\n",
    "vectors = test_df.select(\"vector_avg\").rdd.map(lambda row : row.vector_avg).collect()\n",
    "test_loader = DataLoader(DenseVectorDataset(labels , vectors) , batch_size=128 , shuffle=True)"
   ],
   "id": "b8b91443f079dc99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "build model",
   "id": "5b2a4ca91a6edb51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self , embedding_dim , hidden_dim , output_dim , drop_out = 0.5):\n",
    "        super(Dense, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim , hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.fc2 = nn.Linear(hidden_dim , output_dim)\n",
    "    def forward(self , x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "e50c7c0a76447d1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training model",
   "id": "3d2a8fe9d24fa68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/intent_classification')\n",
    "\n",
    "def fit(model , train_loader , val_loader , optimizer , criterion , epochs = 100 , device  = \"cuda\"):\n",
    "    model.to(device)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        train_loss  = 0.0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "        for x , y in train_loader:\n",
    "            x , y = x.to(device), y.to(device)\n",
    "            # reset gradient\n",
    "            optimizer.zero_grad()\n",
    "            #forward pass\n",
    "            output = model(x)\n",
    "            loss = criterion(output , y)\n",
    "\n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #loss compute\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_train += y.size(0)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        #\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_acc = train_correct / total_train\n",
    "\n",
    "        # valid\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for x , y in val_loader:\n",
    "                x , y = x.to(device), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output , y)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                val_correct += (y==predicted).sum().item()\n",
    "                total_val += y.size(0)\n",
    "        avg_val_acc = val_correct / total_val\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        # logging\n",
    "        writer.add_scalar(\"Loss/Train\" , avg_train_loss , i)\n",
    "        writer.add_scalar(\"Accuracy/Train\" , avg_train_acc , i)\n",
    "        writer.add_scalar(\"Loss/Val\" , avg_val_loss , i)\n",
    "        writer.add_scalar(\"Accuracy/Val\" , avg_val_acc , i)\n",
    "        print(f\"epoch {i} : loss/train: {avg_train_loss:.4f} , acc/train: {avg_train_acc:.4f} \"f\"loss/val: {avg_val_loss:.4f} acc/val:{avg_val_acc:.4f}\")\n"
   ],
   "id": "5a6da9866645ba1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils.class_weight import  compute_class_weight\n",
    "import numpy as np\n",
    "classes = np.unique(labels_train)\n",
    "weights = compute_class_weight(\n",
    "    class_weight='balanced' ,\n",
    "    classes=classes,\n",
    "    y=labels_train\n",
    ")\n",
    "print(weights)"
   ],
   "id": "e755208913be99b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_weights = torch.tensor(weights , dtype = torch.float)\n",
    "class_weights = class_weights.to(\"cuda\")"
   ],
   "id": "96aa26f48a1823d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from torch.utils.\n",
    "model = Dense(embedding_dim=100 , hidden_dim=64 , output_dim=7 , drop_out=0.5)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = Adam(model.parameters() , lr = 1e-3)\n",
    "fit(model , train_loader , val_loader , optimizer , criterion , epochs =100)"
   ],
   "id": "ac7e83fbb07a8a81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test",
   "id": "ada9b134ca63e2e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Dự đoán (Output là Logits)\n",
    "            out = model(x)\n",
    "\n",
    "            # Lấy chỉ số lớp có xác suất cao nhất\n",
    "            _, predicted = torch.max(out, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Giả sử bạn đã chạy đoạn này trong hàm fit, nếu chưa, chạy lại:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# 1. Thu thập nhãn và dự đoán\n",
    "y_true, y_pred = evaluate_metrics(model, test_loader, device = \"cuda\")"
   ],
   "id": "59d847e125ac3fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tính Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Tính F1-Score (Dùng 'macro' để đánh giá công bằng các lớp)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy cuối cùng: {accuracy:.4f}\")\n",
    "print(f\"F1-Macro Score: {f1_macro:.4f}\")\n"
   ],
   "id": "9eac0edb515671cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [\n",
    "    'Anger',     #\n",
    "    'Surprise',  #\n",
    "    'Enjoyment', #\n",
    "    'Other',     #\n",
    "    'Fear',      #\n",
    "    'Sadness',   #\n",
    "    'Disgust'    #\n",
    "]\n",
    "\n",
    "# 1. Tính toán Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 2. Thiết lập kích thước đồ thị\n",
    "plt.figure(figsize=(10, 8)) # Tăng kích thước để dễ đọc tên lớp\n",
    "\n",
    "# 3. Vẽ biểu đồ nhiệt (Heatmap)\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names, # Sử dụng tên lớp mới\n",
    "    yticklabels=class_names  # Sử dụng tên lớp mới\n",
    ")\n",
    "\n",
    "# 4. Đặt tiêu đề và nhãn\n",
    "plt.title('Confusion Matrix: Phân loại Cảm xúc (Emotion Classification)')\n",
    "plt.ylabel('Nhãn Thực tế (True Emotion)')\n",
    "plt.xlabel('Nhãn Dự đoán (Predicted Emotion)')\n",
    "plt.show()"
   ],
   "id": "7460a5f54aed8887",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# embedding + lstm",
   "id": "e6be62de06d56be0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.show(2)",
   "id": "5d68f25edb1852f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:36:30.786262Z",
     "start_time": "2025-11-29T09:36:30.780273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hashing\n",
    "from pyspark.sql.types import IntegerType , ArrayType\n",
    "from pyspark.sql.functions import  udf , lit\n",
    "import torch\n",
    "def hashing(tokens , vocab_size  , max_seq_length):\n",
    "    arr = []\n",
    "    for token in tokens :\n",
    "        raw_idx = hash(token)\n",
    "        mod_idx = raw_idx % vocab_size\n",
    "        if(mod_idx < 0):\n",
    "            mod_idx += vocab_size\n",
    "        arr.append(mod_idx+1)\n",
    "    if(len(arr) < max_seq_length):\n",
    "        arr += [0] * (max_seq_length - len(arr))\n",
    "    else :\n",
    "        arr = arr[:max_seq_length]\n",
    "    return arr\n",
    "hashing_udf = udf(hashing , ArrayType(IntegerType()))"
   ],
   "id": "1573de586cddaeb5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:36:33.248497Z",
     "start_time": "2025-11-29T09:36:33.193570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df_lstm = train_df.withColumn(\"idx\" , hashing_udf(train_df.tokens , lit(3000) , lit(30)))\n",
    "val_df_lstm = val_df.withColumn('idx' , hashing_udf(val_df.tokens , lit(3000) , lit(30)))\n",
    "test_df_lstm = test_df.withColumn('idx' , hashing_udf(test_df.tokens , lit(3000) , lit(30)))"
   ],
   "id": "28740ac30857e49e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df_lstm.head(2)",
   "id": "d02c87a1783ebe1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "csv  =train_df_lstm.toPandas()\n",
    "csv.to_csv(\"train.csv\")"
   ],
   "id": "d1c11ddf47d23661",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(hash(\"ten\")%500)",
   "id": "166d38b815cd5661",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "val_df_lstm.head(2)",
   "id": "89f67704cf4e28b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "data prepare",
   "id": "88ee725341937d57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:36:35.766030Z",
     "start_time": "2025-11-29T09:36:35.762158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader ,Dataset\n",
    "class LSTMData(Dataset):\n",
    "    def __init__(self , indexes   , label):\n",
    "        self.indexes  = torch.tensor(indexes , dtype = torch.long)\n",
    "        self.label = torch.tensor(label , dtype = torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.indexes[idx] , self.label[idx] , sum(self.indexes[idx] != 0)\n"
   ],
   "id": "ec55b1705db037a7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-29T09:36:36.883509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_idx = train_df_lstm.select('idx').rdd.map(lambda x : x.idx).collect()\n",
    "train_label = train_df_lstm.select(\"label\").rdd.map(lambda x : x.label).collect()\n",
    "dataset = LSTMData(train_idx , train_label)\n",
    "train_loader_lstm = DataLoader(dataset,batch_size=128  , num_workers = 4 , shuffle=True)"
   ],
   "id": "c3ca78b85e83e2b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in val_loader_lstm:\n",
    "    print(i[0][:4])\n",
    "    break"
   ],
   "id": "bdf19d5cd1148e7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_idx = val_df_lstm.select(\"idx\").rdd.map(lambda x : x.idx).collect()\n",
    "val_label = train_df_lstm.select(\"label\").rdd.map(lambda x : x.label).collect()\n",
    "dataset = LSTMData(val_idx , val_label)\n",
    "val_loader_lstm = DataLoader(dataset,batch_size=64 , num_workers = 4 , shuffle=True )"
   ],
   "id": "99423d8e58d7721f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_idx = test_df_lstm.select(\"idx\").rdd.map(lambda x : x.idx).collect()\n",
    "test_label = test_df_lstm.select(\"label\").rdd.map(lambda x : x.label).collect()\n",
    "dataset = LSTMData(test_idx , test_label)\n",
    "test_loader_lstm = DataLoader(dataset,batch_size=128 ,  num_workers = 4 , shuffle=True)"
   ],
   "id": "c18a23d76590be01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "build LSTM model",
   "id": "e3c876a55bcfaf19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence , pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "class LSTM_Intent(nn.Module):\n",
    "    def __init__(self ,num_embedding , embedding_dim , hidden_dim , output_dim , n_layers , dropout_rate=0.5):\n",
    "        super(LSTM_Intent, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = num_embedding ,\n",
    "            embedding_dim = embedding_dim ,\n",
    "            padding_idx = 0\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embedding_dim , hidden_dim , num_layers= n_layers , dropout=dropout_rate if n_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(in_features= hidden_dim , out_features= output_dim)\n",
    "        self.batch_norm =nn.BatchNorm1d(hidden_dim)\n",
    "    def forward(self , idx , length ):\n",
    "        out = self.embedding(idx)\n",
    "        # pack\n",
    "        packed = pack_padded_sequence(out , length.cpu() , batch_first=True ,enforce_sorted=False)\n",
    "        out , (h_n , c_n) = self.lstm(packed )\n",
    "        h = h_n[-1]\n",
    "        h = self.batch_norm(h)\n",
    "        h = self.dropout(h)\n",
    "        out = self.fc(h)\n",
    "        return out"
   ],
   "id": "e2367124a5f1bbf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train",
   "id": "83aed9728d425c0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "def fit(model, train_loader, val_loader, optimizer, loss_fn, device=\"cuda\", epochs=200):\n",
    "    writer = SummaryWriter(log_dir='runs/LSTM')\n",
    "\n",
    "    # 1. Chuyển model sang device MỘT LẦN duy nhất ở ngoài vòng lặp\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- TRAIN LOOP ---\n",
    "        model.train() # Bật chế độ train (quan trọng cho Dropout, BatchNorm)\n",
    "        train_loss = 0.0\n",
    "        acc = 0\n",
    "        total = 0\n",
    "\n",
    "        for x, y, length in train_loader:\n",
    "            # 2. Tối ưu: Không chuyển length sang GPU nếu model đã tự xử lý .cpu()\n",
    "            # Hoặc chỉ chuyển x và y\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # length = length.to(device) # Bỏ dòng này nếu model đã handle CPU\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x, length) # Truyền length (vẫn đang ở CPU hoặc GPU tùy bạn xử lý)\n",
    "            loss = loss_fn(out, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            acc += (y == predicted).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        acc_avg = acc / total\n",
    "        loss_avg = train_loss / len(train_loader)\n",
    "\n",
    "        # --- VAL LOOP ---\n",
    "        model.eval() # Bật chế độ eval\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0\n",
    "        val_total = 0\n",
    "\n",
    "        # 3. QUAN TRỌNG: Dùng no_grad để tiết kiệm bộ nhớ và tăng tốc\n",
    "        with torch.no_grad():\n",
    "            for x, y, length in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                out = model(x, length)\n",
    "                ValLoss = loss_fn(out, y)\n",
    "\n",
    "                _, val_predicted = torch.max(out, 1)\n",
    "                val_loss += ValLoss.item()\n",
    "                val_acc += (val_predicted == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        val_acc_avg = val_acc / val_total\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "\n",
    "        # Log tensorboard\n",
    "        writer.add_scalar(\"Train/Loss\", loss_avg, epoch)\n",
    "        writer.add_scalar(\"Train/Accuracy\", acc_avg, epoch)\n",
    "        writer.add_scalar(\"Val/Loss\", val_loss_avg, epoch)\n",
    "        writer.add_scalar(\"Val/Accuracy\", val_acc_avg, epoch)\n",
    "\n",
    "        # In ra màn hình để theo dõi tiến độ (Optional)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {loss_avg:.4f} | Val Loss: {val_loss_avg:.4f} | Val Acc: {val_acc_avg:.4f} | Train Acc: {acc_avg:.4f}\")\n",
    "\n",
    "    # 4. Đóng writer\n",
    "    writer.close()"
   ],
   "id": "15c0462822034055",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils.class_weight import  compute_class_weight\n",
    "import numpy as np\n",
    "classes = np.unique(train_label)\n",
    "weights = compute_class_weight(\n",
    "    class_weight='balanced' ,\n",
    "    classes=classes,\n",
    "    y=train_label\n",
    ")\n",
    "print(weights)"
   ],
   "id": "eee22464f59844ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_weights = torch.tensor(weights , dtype = torch.float)\n",
    "class_weights = class_weights.to(\"cuda\")"
   ],
   "id": "f06a0d50170aca3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from torch.utils.\n",
    "model = LSTM_Intent(num_embedding=3000 , embedding_dim=128 , hidden_dim=64 , output_dim=7 , n_layers=2, dropout_rate=0.4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights , ignore_index=0)\n",
    "optimizer = Adam(model.parameters() , lr = 1e-3, weight_decay = 1e-3)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fit(model , train_loader_lstm , val_loader_lstm , optimizer  , criterion , epochs =100)"
   ],
   "id": "7a368ab436bfbd38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, length in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Dự đoán (Output là Logits)\n",
    "            out = model(x, length)\n",
    "\n",
    "            # Lấy chỉ số lớp có xác suất cao nhất\n",
    "            _, predicted = torch.max(out, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Giả sử bạn đã chạy đoạn này trong hàm fit, nếu chưa, chạy lại:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# 1. Thu thập nhãn và dự đoán\n",
    "y_true, y_pred = evaluate_metrics(model, test_loader_lstm, device)"
   ],
   "id": "79e9454d7d04bd03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tính Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Tính F1-Score (Dùng 'macro' để đánh giá công bằng các lớp)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy cuối cùng: {accuracy:.4f}\")\n",
    "print(f\"F1-Macro Score: {f1_macro:.4f}\")\n"
   ],
   "id": "7209e58b70c9f06c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Thay thế bằng danh sách tên cảm xúc theo thứ tự nhãn (thường là 0 đến 6)\n",
    "# **RẤT QUAN TRỌNG: Thứ tự này phải khớp với cách bạn mã hóa các nhãn y (target labels) ban đầu.**\n",
    "class_names = [\n",
    "    'Anger',     #\n",
    "    'Surprise',  #\n",
    "    'Enjoyment', #\n",
    "    'Other',     #\n",
    "    'Fear',      #\n",
    "    'Sadness',   #\n",
    "    'Disgust'    #\n",
    "]\n",
    "\n",
    "# 1. Tính toán Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 2. Thiết lập kích thước đồ thị\n",
    "plt.figure(figsize=(10, 8)) # Tăng kích thước để dễ đọc tên lớp\n",
    "\n",
    "# 3. Vẽ biểu đồ nhiệt (Heatmap)\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names, # Sử dụng tên lớp mới\n",
    "    yticklabels=class_names  # Sử dụng tên lớp mới\n",
    ")\n",
    "\n",
    "# 4. Đặt tiêu đề và nhãn\n",
    "plt.title('Confusion Matrix: Phân loại Cảm xúc (Emotion Classification)')\n",
    "plt.ylabel('Nhãn Thực tế (True Emotion)')\n",
    "plt.xlabel('Nhãn Dự đoán (Predicted Emotion)')\n",
    "plt.show()"
   ],
   "id": "af0324768ee0c23f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:39:34.615193Z",
     "start_time": "2025-11-29T09:39:31.002414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0)\n"
   ],
   "id": "c2dfcbe3f8a0f252",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
