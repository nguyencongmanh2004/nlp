{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-08T14:07:12.620166Z",
     "start_time": "2025-11-08T14:07:12.617425Z"
    }
   },
   "source": [
    "from nltk import regexp_tokenize\n",
    "from pyspark.sql.functions.builtin import regexp_extract_all\n",
    "\n",
    "path = '/home/manh/code/nlp/src/data/sentiments.csv'"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:09:35.433217Z",
     "start_time": "2025-11-08T14:09:29.935831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")"
   ],
   "id": "fa0683f7ad172821",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:09:59.645892Z",
     "start_time": "2025-11-08T14:09:59.599985Z"
    }
   },
   "cell_type": "code",
   "source": "ds.to_csv(\"sentiments.csv\", index=False)",
   "id": "33883b62ddf46edf",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentiments.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DatasetDict' object has no attribute 'to_csv'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:06:19.750079Z",
     "start_time": "2025-11-08T14:06:14.325377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadCSVExample\") \\\n",
    "    .getOrCreate()\n"
   ],
   "id": "de8409099bb9719",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/08 21:06:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:07:18.867439Z",
     "start_time": "2025-11-08T14:07:16.887565Z"
    }
   },
   "cell_type": "code",
   "source": "df =  spark.read.csv(path , header = True , inferSchema= True)",
   "id": "d3bc924bb6ab262",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T14:08:11.638534Z",
     "start_time": "2025-11-08T14:08:10.766104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df.groupby(\"sentiment\").count()\n",
    "\n",
    "df.show()"
   ],
   "id": "470e77561cd78730",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|       -1|    1|\n",
      "|        1|    1|\n",
      "|     NULL|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:24:54.708412Z",
     "start_time": "2025-11-04T07:24:54.555956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ff = df.na.drop(subset=['sentiment'])\n",
    "ff.show(3)"
   ],
   "id": "a3289c191ea0c696",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "|Kickers on my wat...|        1|\n",
      "|user: AAP MOVIE. ...|        1|\n",
      "|user I'd be afrai...|        1|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:24:54.757104Z",
     "start_time": "2025-11-04T07:24:54.734315Z"
    }
   },
   "cell_type": "code",
   "source": "train , test = ff.randomSplit([0.8 , 0.2] , seed = 42)",
   "id": "805a5a6ae35d626c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:24:54.856155Z",
     "start_time": "2025-11-04T07:24:54.784054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"indexed_sentiment\")\n",
    "# indexed = indexer.fit(ff).transform(ff)"
   ],
   "id": "e8e44c809d8d3a46",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:24:54.873724Z",
     "start_time": "2025-11-04T07:24:54.861999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol =\"text\", outputCol=\"token\" ,pattern = r'[! ,\\-:;?)(\"\\'\\n]+' , toLowercase  = True)\n"
   ],
   "id": "ae0160a83aadadc3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:24:54.986619Z",
     "start_time": "2025-11-04T07:24:54.911132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol = 'token' , outputCol = \"token_removed\")"
   ],
   "id": "fd512d27ead67e41",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#print(hello world)\n",
   "id": "838721937b372d50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:24:55.004891Z",
     "start_time": "2025-11-04T07:24:54.993640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"token_removed\" , outputCol=\"features\" , minDF=10)\n"
   ],
   "id": "71c0981b93a10942",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:24:55.092209Z",
     "start_time": "2025-11-04T07:24:55.042911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "logistic = LogisticRegression(featuresCol=\"features\", labelCol=\"indexed_sentiment\")\n"
   ],
   "id": "88e3ad31bd0734c9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:25:03.193369Z",
     "start_time": "2025-11-04T07:24:55.099629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [tokenizer ,indexer , remover ,  vectorizer , logistic])\n",
    "\n",
    "model = pipeline.fit(train)"
   ],
   "id": "9b32922b611065f7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 14:24:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:25:03.589762Z",
     "start_time": "2025-11-04T07:25:03.255622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.show(2)"
   ],
   "id": "90e32cf4c8cb081d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|sentiment|               token|indexed_sentiment|       token_removed|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  ISG An update t...|       -1|[isg, an, update,...|              1.0|[isg, update, feb...|(804,[59,124,201,...|[-4.2542562049815...|[0.01400473307589...|       1.0|\n",
      "|  The rodeo clown...|       -1|[the, rodeo, clow...|              1.0|[rodeo, clown, se...|(804,[33,141,224,...|[-3.7505890025280...|[0.02296415086506...|       1.0|\n",
      "+--------------------+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:25:03.883549Z",
     "start_time": "2025-11-04T07:25:03.620995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"indexed_sentiment\",\n",
    "    predictionCol = \"prediction\",\n",
    "    metricName = \"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy = accuracy * 100\n",
    "print(accuracy)\n"
   ],
   "id": "fee97771eca75bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.27682596934174\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
