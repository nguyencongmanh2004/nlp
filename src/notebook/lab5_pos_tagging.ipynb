{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe62027adfdf84ae",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2d98aa5b1061da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read conllu file\n",
    "\n",
    "from conllu import parse , parse_incr\n",
    "def load_conllu(path):\n",
    "    \"\"\" return list tuple(word ,upos_tag) \"\"\"\n",
    "    result = []\n",
    "    with open(path , 'r' , encoding='utf-8') as f:\n",
    "        # data = f.read()\n",
    "        for obj in parse_incr(f):\n",
    "            list_sentence = []\n",
    "            for token in obj:\n",
    "                if isinstance(token['id'], int):\n",
    "                    word = token['form']\n",
    "                    upos_tag = token['upos']\n",
    "                    list_sentence.append((word , upos_tag))\n",
    "            result.append(list_sentence)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c1a4916dbbefe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = r'/home/manh/code/nlp/src/data/UD_English-EWT/en_ewt-ud-train.conllu'\n",
    "data = load_conllu(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be972c",
   "metadata": {},
   "source": [
    "tạo từ điển cho words và tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c295c9ba24c583d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic(data):\n",
    "    # 1. Khai báo PAD là 0, UNK là 1 ngay từ đầu\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'<PAD>': 0}\n",
    "\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            token = word[0] # Từ\n",
    "            tag = word[1]   # Nhãn\n",
    "\n",
    "            # Logic tăng index chuẩn: Lấy độ dài hiện tại làm index mới\n",
    "            if token not in word_to_idx:\n",
    "                word_to_idx[token] = len(word_to_idx)\n",
    "\n",
    "            if tag not in tag_to_idx:\n",
    "                tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "    return word_to_idx, tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd5cf90f05e4533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len word dict : 19675 , len tag dict : 18\n"
     ]
    }
   ],
   "source": [
    "word_idx , tag_idx = dic(data)\n",
    "print(f\"len word dict : {len(word_idx)} , len tag dict : {len(tag_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b41fcabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " 'PROPN': 1,\n",
       " 'PUNCT': 2,\n",
       " 'ADJ': 3,\n",
       " 'NOUN': 4,\n",
       " 'VERB': 5,\n",
       " 'DET': 6,\n",
       " 'ADP': 7,\n",
       " 'AUX': 8,\n",
       " 'PRON': 9,\n",
       " 'PART': 10,\n",
       " 'SCONJ': 11,\n",
       " 'NUM': 12,\n",
       " 'ADV': 13,\n",
       " 'CCONJ': 14,\n",
       " 'X': 15,\n",
       " 'INTJ': 16,\n",
       " 'SYM': 17}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e29b64a480672",
   "metadata": {},
   "source": [
    "train loader , dev loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4897ecc442a7be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, word_idx, tag_idx):\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "\n",
    "    unk_idx = word_idx.get('<UNK>', 1) # Lấy index UNK an toàn\n",
    "\n",
    "    for sentence in data:\n",
    "        x_idx = []\n",
    "        y_idx = []\n",
    "        for token in sentence:\n",
    "            # Xử lý từ (Input)\n",
    "            word = token[0]\n",
    "            if word in word_idx:\n",
    "                x_idx.append(word_idx[word])\n",
    "            else:\n",
    "                x_idx.append(unk_idx)\n",
    "\n",
    "            # Xử lý nhãn (Label) - Nhãn trong train phải luôn tồn tại\n",
    "            tag = token[1]\n",
    "            # Nếu lỡ có tag lạ trong dev set thì cho về <PAD> hoặc 1 tag mặc định\n",
    "            if tag in tag_idx:\n",
    "                y_idx.append(tag_idx[tag])\n",
    "            else:\n",
    "                # Trường hợp hiếm: tag lạ -> bỏ qua hoặc gán đại\n",
    "                y_idx.append(0)\n",
    "\n",
    "        X_data.append(x_idx)\n",
    "        Y_data.append(y_idx)\n",
    "\n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb21fd7",
   "metadata": {},
   "source": [
    "dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f9972e6b1c2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader , Dataset\n",
    "import torch\n",
    "class POSDataset(Dataset):\n",
    "    def __init__(self, X_data , Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_data[idx], self.Y_data[idx] , len(self.X_data[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2b32f",
   "metadata": {},
   "source": [
    "padding setence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8222dd001393bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 1. Chuyển list thường thành Tensor (để sửa lỗi TypeError ban nãy)\n",
    "    # Lưu ý: Nếu Dataset của bạn đã trả về Tensor rồi thì bước này vẫn chạy tốt (không ảnh hưởng)\n",
    "    sentences = [torch.tensor(item[0], dtype=torch.long) if not torch.is_tensor(item[0]) else item[0] for item in batch]\n",
    "    tags = [torch.tensor(item[1], dtype=torch.long) if not torch.is_tensor(item[1]) else item[1] for item in batch]\n",
    "\n",
    "    # 2. Lấy độ dài thực tế (Length) TRƯỚC khi padding\n",
    "    # Đây là cái bạn đang thiếu\n",
    "    lengths = torch.tensor([len(s) for s in sentences], dtype=torch.long)\n",
    "\n",
    "    # 3. Thực hiện Padding\n",
    "    padding_value = 0\n",
    "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=padding_value)\n",
    "    padded_tags = pad_sequence(tags, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    # 4. Trả về đủ 3 giá trị\n",
    "    return padded_sentences, padded_tags, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b324c9e5ea367222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loader\n",
    "train_file = r'/home/manh/code/nlp/src/data/UD_English-EWT/en_ewt-ud-train.conllu'\n",
    "train_data = load_conllu(train_file)\n",
    "X_train , Y_train = transform_data(train_data , word_idx, tag_idx)\n",
    "train_dataset = POSDataset(X_train , Y_train)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size = 64 ,\n",
    "    shuffle = True,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b6b38",
   "metadata": {},
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a02c359c2b1d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file = r'//home/manh/code/nlp/src/data/UD_English-EWT/en_ewt-ud-dev.conllu'\n",
    "dev_data = load_conllu(dev_file)\n",
    "X_dev , Y_dev = transform_data(dev_data , word_idx, tag_idx)\n",
    "dev_dataset = POSDataset(X_dev , Y_dev)\n",
    "dev_loader = DataLoader(\n",
    "    dataset=dev_dataset,\n",
    "    batch_size = 64 ,\n",
    "    shuffle = True,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8153b",
   "metadata": {},
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877f7652aed88ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = r'/home/manh/code/nlp/src/data/UD_English-EWT/en_ewt-ud-test.conllu'\n",
    "test_data = load_conllu(test_file)\n",
    "X_test, Y_test = transform_data(test_data , word_idx, tag_idx)\n",
    "test_dataset = POSDataset(X_test , Y_test)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size = 64 ,\n",
    "    shuffle = True,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471585e16302890",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "356dd1e86e125fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size , embedding_dim, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_dim , padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim , hidden_size , batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size , output_size)\n",
    "    def forward(self , x , length):\n",
    "        x = self.embedding(x)\n",
    "        packed = pack_padded_sequence(x, length, enforce_sorted=False , batch_first=True)\n",
    "        packed_output , h_n = self.rnn(packed)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        logit = self.fc(rnn_out)\n",
    "        return logit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a8e2e03056770",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a627b07a407c3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"run/posTagging\")\n",
    "def fit(model , train_loader , val_loader , optimizer , criterion  , device = \"cuda\" , epochs = 200):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        loss_train = 0.0\n",
    "        model.train()\n",
    "        for x ,y , length in train_loader:\n",
    "            x , y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model.forward(x , length)\n",
    "            #outshape (batch size , seq_len , num_class )\n",
    "\n",
    "            # cross entropy loss require\n",
    "            # input shape ( batch size * sqe_len  , num_class)\n",
    "            # taget (batch_size * sqe_len)\n",
    "            out_reshaped = out.view(-1 , out.shape[-1])\n",
    "            y_reshaped = y.view(-1)\n",
    "            loss = criterion(out_reshaped, y_reshaped)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # loss , acc calculate\n",
    "            _ , predicted = torch.max(out , dim=2)\n",
    "            correct_train += (predicted==y).sum().item()\n",
    "            loss_train += loss.item()\n",
    "            mask = (y!=0)\n",
    "            total_train += mask.sum().item()\n",
    "\n",
    "        acc_avg = correct_train / total_train\n",
    "        loss_avg = loss_train / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss_avg:.4f} | Acc: {acc_avg:.4f}\")\n",
    "\n",
    "        # (Optional) Thêm phần đánh giá trên dev_loader ở đây...\n",
    "        writer.add_scalar(\"Loss/train\", loss_avg, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", acc_avg, epoch)\n",
    "        # val test\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        loss_val= 0.0\n",
    "        model.eval()\n",
    "        for x , y , length in val_loader:\n",
    "            x , y = x.to(device), y.to(device)\n",
    "            out = model.forward(x , length)\n",
    "            out_reshaped = out.view(-1 , out.shape[-1])\n",
    "            y_reshaped = y.view(-1)\n",
    "            loss = criterion(out_reshaped, y_reshaped)\n",
    "            _ , predicted = torch.max(out , dim=2)\n",
    "\n",
    "            correct_val += (predicted==y).sum().item()\n",
    "            loss_val += loss.item()\n",
    "            mask = (y!=0)\n",
    "            total_val += mask.sum().item()\n",
    "        avg_acc_val = correct_val / total_val\n",
    "        avg_loss_val = loss_val / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss_val: {avg_loss_val:.4f} | Acc_val: {avg_acc_val:.4f}\")\n",
    "\n",
    "        # (Optional) Thêm phần đánh giá trên dev_loader ở đây...\n",
    "        writer.add_scalar(\"Loss/val\", avg_loss_val, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val\", avg_acc_val, epoch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2ce37",
   "metadata": {},
   "source": [
    "Hàm evaluate cho tập test tính accuracy và loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5718a2e52241903e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:13:51.117986Z",
     "start_time": "2025-12-06T03:13:51.113507Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model , dev_loader , optimizer , criterion , device = \"cuda\" ):\n",
    "    total_cor = 0\n",
    "    loss_test = 0.0\n",
    "    total = 0\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x , y , length in dev_loader:\n",
    "            x , y = x.to(device), y.to(device)\n",
    "            out = model.forward(x , length)\n",
    "            out_reshaped = out.view(-1 , out.shape[-1])\n",
    "            y_reshaped = y.view(-1)\n",
    "            loss = criterion(out_reshaped, y_reshaped)\n",
    "\n",
    "            _ , predicted = torch.max(out , dim=2)\n",
    "            total_cor += (predicted == y).sum().item()\n",
    "            total += (y!=0).sum().item()\n",
    "            loss_test += loss.item()\n",
    "        avg_loss = loss_test / len(dev_loader)\n",
    "        avg_acc = total_cor / total\n",
    "        print(f\"test loss  {avg_loss:.4f} | test acc: {avg_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0865682e3338f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:12:43.804308Z",
     "start_time": "2025-12-06T03:11:33.542331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Loss: 1.2593 | Acc: 0.6267\n",
      "Epoch 1/30 | Loss_val: 0.8874 | Acc_val: 0.7286\n",
      "Epoch 2/30 | Loss: 0.6873 | Acc: 0.7826\n",
      "Epoch 2/30 | Loss_val: 0.7154 | Acc_val: 0.7881\n",
      "Epoch 3/30 | Loss: 0.5201 | Acc: 0.8358\n",
      "Epoch 3/30 | Loss_val: 0.6351 | Acc_val: 0.8184\n",
      "Epoch 4/30 | Loss: 0.4169 | Acc: 0.8679\n",
      "Epoch 4/30 | Loss_val: 0.5974 | Acc_val: 0.8343\n",
      "Epoch 5/30 | Loss: 0.3435 | Acc: 0.8909\n",
      "Epoch 5/30 | Loss_val: 0.5750 | Acc_val: 0.8463\n",
      "Epoch 6/30 | Loss: 0.2882 | Acc: 0.9085\n",
      "Epoch 6/30 | Loss_val: 0.5838 | Acc_val: 0.8562\n",
      "Epoch 7/30 | Loss: 0.2447 | Acc: 0.9222\n",
      "Epoch 7/30 | Loss_val: 0.5825 | Acc_val: 0.8609\n",
      "Epoch 8/30 | Loss: 0.2088 | Acc: 0.9344\n",
      "Epoch 8/30 | Loss_val: 0.5897 | Acc_val: 0.8658\n",
      "Epoch 9/30 | Loss: 0.1798 | Acc: 0.9430\n",
      "Epoch 9/30 | Loss_val: 0.6187 | Acc_val: 0.8662\n",
      "Epoch 10/30 | Loss: 0.1551 | Acc: 0.9512\n",
      "Epoch 10/30 | Loss_val: 0.6447 | Acc_val: 0.8685\n",
      "Epoch 11/30 | Loss: 0.1347 | Acc: 0.9576\n",
      "Epoch 11/30 | Loss_val: 0.7015 | Acc_val: 0.8705\n",
      "Epoch 12/30 | Loss: 0.1164 | Acc: 0.9631\n",
      "Epoch 12/30 | Loss_val: 0.6805 | Acc_val: 0.8707\n",
      "Epoch 13/30 | Loss: 0.1013 | Acc: 0.9686\n",
      "Epoch 13/30 | Loss_val: 0.7193 | Acc_val: 0.8708\n",
      "Epoch 14/30 | Loss: 0.0885 | Acc: 0.9725\n",
      "Epoch 14/30 | Loss_val: 0.7620 | Acc_val: 0.8721\n",
      "Epoch 15/30 | Loss: 0.0770 | Acc: 0.9764\n",
      "Epoch 15/30 | Loss_val: 0.7767 | Acc_val: 0.8722\n",
      "Epoch 16/30 | Loss: 0.0676 | Acc: 0.9792\n",
      "Epoch 16/30 | Loss_val: 0.8131 | Acc_val: 0.8715\n",
      "Epoch 17/30 | Loss: 0.0592 | Acc: 0.9820\n",
      "Epoch 17/30 | Loss_val: 0.8487 | Acc_val: 0.8713\n",
      "Epoch 18/30 | Loss: 0.0517 | Acc: 0.9843\n",
      "Epoch 18/30 | Loss_val: 0.8488 | Acc_val: 0.8717\n",
      "Epoch 19/30 | Loss: 0.0455 | Acc: 0.9865\n",
      "Epoch 19/30 | Loss_val: 0.9142 | Acc_val: 0.8719\n",
      "Epoch 20/30 | Loss: 0.0398 | Acc: 0.9886\n",
      "Epoch 20/30 | Loss_val: 0.9576 | Acc_val: 0.8704\n",
      "Epoch 21/30 | Loss: 0.0348 | Acc: 0.9904\n",
      "Epoch 21/30 | Loss_val: 0.9278 | Acc_val: 0.8712\n",
      "Epoch 22/30 | Loss: 0.0307 | Acc: 0.9911\n",
      "Epoch 22/30 | Loss_val: 0.9750 | Acc_val: 0.8697\n",
      "Epoch 23/30 | Loss: 0.0272 | Acc: 0.9926\n",
      "Epoch 23/30 | Loss_val: 0.9711 | Acc_val: 0.8689\n",
      "Epoch 24/30 | Loss: 0.0246 | Acc: 0.9930\n",
      "Epoch 24/30 | Loss_val: 1.0119 | Acc_val: 0.8680\n",
      "Epoch 25/30 | Loss: 0.0216 | Acc: 0.9940\n",
      "Epoch 25/30 | Loss_val: 1.0562 | Acc_val: 0.8682\n",
      "Epoch 26/30 | Loss: 0.0196 | Acc: 0.9943\n",
      "Epoch 26/30 | Loss_val: 1.0625 | Acc_val: 0.8690\n",
      "Epoch 27/30 | Loss: 0.0179 | Acc: 0.9948\n",
      "Epoch 27/30 | Loss_val: 1.0869 | Acc_val: 0.8682\n",
      "Epoch 28/30 | Loss: 0.0163 | Acc: 0.9951\n",
      "Epoch 28/30 | Loss_val: 1.1187 | Acc_val: 0.8683\n",
      "Epoch 29/30 | Loss: 0.0154 | Acc: 0.9953\n",
      "Epoch 29/30 | Loss_val: 1.1755 | Acc_val: 0.8675\n",
      "Epoch 30/30 | Loss: 0.0149 | Acc: 0.9952\n",
      "Epoch 30/30 | Loss_val: 1.1780 | Acc_val: 0.8667\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocab_size=20000 , embedding_dim=128 , hidden_size=128 , output_size=18)\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "fit(model , train_loader ,dev_loader , optimizer , criterion , device = \"cuda\" , epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23985708b859077f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:13:54.938037Z",
     "start_time": "2025-12-06T03:13:54.782054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss  1.1952 | test acc: 0.8667\n"
     ]
    }
   ],
   "source": [
    "evaluate(model , test_loader , optimizer , criterion , device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1229d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[16.3601, 21.0545, 13.9500]], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[9, 5, 4]], device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"i love nlp\".split()\n",
    "\n",
    "# 1) Chuyển từ sang index\n",
    "idxs = [word_idx.get(w, 1) for w in sentence]\n",
    "\n",
    "# 2) Tensor index (GPU)\n",
    "tensor = torch.tensor(idxs, dtype=torch.long).unsqueeze(0).to('cuda')\n",
    "\n",
    "# 3) Tensor lengths (CPU, 1D)\n",
    "lengths = torch.tensor([len(idxs)], dtype=torch.long)   \n",
    "\n",
    "# 4) Run model\n",
    "output = model(tensor, lengths)\n",
    "y = torch.max(output , dim=2)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c4e07",
   "metadata": {},
   "source": [
    "Mapping kết quả ta được .\n",
    " | Từ       | POS (nhãn)     | Index trong word_idx |\n",
    "| -------- | -------------- | -------------------- |\n",
    "| **i**    | PRON (đại từ)  | **9**                |\n",
    "| **love** | VERB (động từ) | **5**                |\n",
    "| **nlp**  | NOUN (danh từ) | **4**                |\n",
    "\n",
    "vậy kết quả thu được là hoàn toàn chính xác \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e23eb53",
   "metadata": {},
   "source": [
    "Đánh giá kết quả  mô hình "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b1a79",
   "metadata": {},
   "source": [
    "| Tập dữ liệu             | Loss       | Accuracy   |\n",
    "| ----------------------- | ---------- | ---------- |\n",
    "| **Train (Epoch 30/30)** | **0.0149** | **0.9952** |\n",
    "| **Validation**          | **1.1780** | **0.8667** |\n",
    "| **Test**                | **1.1952** | **0.8667** |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
